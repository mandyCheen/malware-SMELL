import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm

from torchvision.models import mobilenet_v2


class MarkerLayer(nn.Module):
    def __init__(self, output_dim, input_dim=None, pos_markers=3, markers=4, weights=None, alpha=1.0, distance=8):
        super(MarkerLayer, self).__init__()
        self.output_dim = output_dim
        self.input_dim = input_dim
        self.alpha = alpha
        self.distance = distance
        self.pos_markers = pos_markers
        self.markers = markers
        
        if weights is None:
            self.W = nn.Parameter(torch.randn(markers, input_dim))
        else:
            self.W = nn.Parameter(torch.tensor(weights, dtype=torch.float))
        
    def forward(self, x):
        num_pos = self.pos_markers

        x = x.unsqueeze(1)
        W = self.W.unsqueeze(0)
        q = 1.0 / (1.0 + torch.sqrt(torch.sum((x - W)**2, dim=2)) ** 2 / self.alpha)
        q = q ** ((self.alpha + 1.0) / 2.0)
        q = q / torch.sum(q, dim=1, keepdim=True)
        
        pos = torch.sum(q[:, :num_pos], dim=1, keepdim=True)
        neg = torch.sum(q[:, num_pos:], dim=1, keepdim=True)
        
        q_new = torch.cat([neg, pos], dim=1)
        q_new = q_new.transpose(0, 1).reshape(2, -1)
        
        return q_new
    
class MarkerLayer_ZSL(nn.Module):
    def __init__(self, output_dim, input_dim=None, pos_markers=3, markers=4, weights=None, alpha=1.0, distance=8):
        super(MarkerLayer_ZSL, self).__init__()
        self.output_dim = output_dim
        self.input_dim = input_dim
        self.alpha = alpha
        self.distance = distance
        self.pos_markers = pos_markers
        self.markers = markers
        
        if weights is None:
            self.W = nn.Parameter(torch.randn(markers, input_dim))
        else:
            self.W = nn.Parameter(torch.tensor(weights, dtype=torch.float))
        
    def forward(self, x):
        num_pos = self.pos_markers

        x = x.unsqueeze(1)
        W = self.W.unsqueeze(0)
        dist = torch.sum((x - W)**2, dim=2)
        sqrt_dist = torch.sqrt(dist)

        q = 1.0 / (1.0 + torch.sqrt(torch.sum((x - W)**2, dim=2)) ** 2 / self.alpha)
        q = q ** ((self.alpha + 1.0) / 2.0)
        q = q / torch.sum(q, dim=1, keepdim=True)
        
        posDist = torch.sum(sqrt_dist[:, :num_pos], dim=1, keepdim=True)
        negDist = torch.sum(sqrt_dist[:, num_pos:], dim=1, keepdim=True)
        
        q_new = torch.cat([negDist, posDist], dim=1)
        q_new = q_new.transpose(0, 1).reshape(2, -1)
        
        return q_new

class Autoencoder(nn.Module):
    def __init__(self, input_channels=1, num_classes=25, pretrained=False) -> None:
        super().__init__()

        # 加载预训练的MobileNetV2模型
        self.mobilenet = mobilenet_v2(pretrained=False)
        
        # 修改第一层卷积以接受单通道输入
        # 原始MobileNetV2的第一层卷积默认接受3个通道的输入
        self.mobilenet.features[0][0] = nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1, bias=False)
        
        # 修改分类器，适应指定的输出类别数
        in_features = self.mobilenet.classifier[1].in_features  # 获取原始全连接层的输入特征数量
        self.mobilenet.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(in_features, num_classes)
        )
    
    def forward(self, x):
        # 定义前向传播
        return self.mobilenet(x)


class Encoder(nn.Module):
    def __init__(self, autoencoder, encoding_dim=128):
        super(Encoder, self).__init__()
        # 使用 autoencoder 的特征提取部分
        self.features = autoencoder.mobilenet.features
        # 添加一个新的全连接层来生成编码输出
        in_features = autoencoder.mobilenet.classifier[1].in_features
        self.encoder = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # 将输出调整为 (1, 1) 大小
            nn.Flatten(),  # 展平特征
            nn.Linear(in_features, encoding_dim),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.features(x)
        x = self.encoder(x)
        return x

class SiameseNetwork(nn.Module):
    def __init__(self, encoder, autoencoder, cluster_layer):
        super(SiameseNetwork, self).__init__()
        self.encoder = encoder
        self.autoencoder = autoencoder
        self.cluster_layer = cluster_layer  # 这是处理L1距离后的额外层

    def forward(self, left_input, right_input):
        # 分别通过编码器和自编码器处理输入
        encoded_l = self.encoder(left_input)
        encoded_r = self.encoder(right_input)
        autoencoded_l = self.autoencoder(left_input)
        autoencoded_r = self.autoencoder(right_input)

        # 计算L1距离
        L1_distance = torch.abs(encoded_l - encoded_r)

        # 可选的聚类层处理
        output = self.cluster_layer(L1_distance)
        return output
    


class SiameseDataset(Dataset):
    def __init__(self, X, num, transform=None, zsl=False):
        self.X = X
        self.num_samples = num
        self.transform = transform
        self.zsl = zsl
        self.pairs, self.labels = self.create_pairs()
    
    def create_pairs(self):
        pairs = [[], []]
        labels = []

        if self.zsl:
            for i in range(self.num_samples):
                r = 0
                idx1, idx2 = np.random.choice(self.X[r].shape[0], 2, replace=False)
                pairs[0].append(self.X[r][idx1])
                pairs[1].append(self.X[r][idx2])
                labels.append(1)
        else:
            for i in range(self.num_samples):
                if i < self.num_samples / 2:
                    # Create positive pair
                    r = np.random.randint(self.X.shape[0])
                    idx1, idx2 = np.random.choice(self.X[r].shape[0], 2, replace=False)
                    pairs[0].append(self.X[r][idx1])
                    pairs[1].append(self.X[r][idx2])
                    labels.append(1)
                else:
                    # Create negative pair
                    r1, r2 = np.random.choice(self.X.shape[0], 2, replace=False)
                    idx1 = np.random.randint(self.X[r1].shape[0])
                    idx2 = np.random.randint(self.X[r2].shape[0])
                    pairs[0].append(self.X[r1][idx1])
                    pairs[1].append(self.X[r2][idx2])
                    labels.append(0)

        return (np.array(pairs[0]), np.array(pairs[1])), np.array(labels)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        x1, x2 = self.pairs[0][idx], self.pairs[1][idx]
        y = self.labels[idx]

        if self.transform:
            x1 = self.transform(x1)
            x2 = self.transform(x2)
        x1 = torch.tensor(x1, dtype=torch.float32)
        x2 = torch.tensor(x2, dtype=torch.float32)
        
        return x1, x2, y

class SiameseDatasetTest(Dataset):
    def __init__(self, x_train_fsl, x_test_sample):
        self.x_train_fsl = x_train_fsl
        self.x_test_sample = x_test_sample

    def __len__(self):
        return len(self.x_train_fsl)

    def __getitem__(self, idx):
        x_train_sample = self.x_train_fsl[idx]
        return torch.tensor(x_train_sample, dtype=torch.float32), torch.tensor(self.x_test_sample, dtype=torch.float32)



class SMELL():
    def __init__(self,
                 n_clusters, 
                 pos_markers, 
                 input_shape, 
                 r_kl=1, 
                 r_r=0.001, 
                 alpha=1.0, 
                 autoencoder_training=400, 
                 pretrained_weights=None, 
                 cluster_centres=None, 
                 batch_size=32, 
                 verbose=0,
                 device='cpu',
                 zsl=False):
        super(SMELL, self).__init__()

        self.n_clusters = n_clusters
        self.input_shape = input_shape
        self.alpha = alpha
        self.pretrained_weights = pretrained_weights
        self.cluster_centres = cluster_centres
        self.batch_size = batch_size
        self.autoencoder_training = autoencoder_training
        self.learning_rate = 0.1
        # self.iters_lr_update = 20000
        self.lr_change_rate = 0.1
        self.verbose = verbose
        self.device = device
        self.pos_markers = pos_markers
        self.r_kl = r_kl
        self.r_r = r_r
        self.zsl = zsl
        
        self.autoencoder = Autoencoder()
        
        if cluster_centres is not None:
                    assert cluster_centres.shape[0] == self.n_clusters
                    assert cluster_centres.shape[1] == self.encoder.layers[-1].output_dim

        if pretrained_weights is not None:
            self.autoencoder.load_state_dict(torch.load(self.pretrained_weights))


    def initialize(self, X, y, save_autoencoder=False, v_train=None, save_path="./autoencoder.pth"):
        if self.pretrained_weights is None:
            dataset = TensorDataset(torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.long))
            dataloader = DataLoader(dataset, batch_size=256)
            
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(self.autoencoder.parameters())
            
            print('Finetuning autoencoder')
            self.autoencoder.train()

            for epoch in range(self.autoencoder_training):
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    output = self.autoencoder(batch_X.unsqueeze(1))
                    output = output.view(output.size(0), -1)
                    loss = criterion(output, batch_y)
                    loss.backward()
                    optimizer.step()
                
                if self.verbose != 0 and (epoch+1) % 10 == 0:
                    print(f'Epoch [{epoch+1}/{self.autoencoder_training}], Loss: {loss.item():.4f}')
            
            if save_autoencoder:
                torch.save(self.autoencoder.state_dict(), save_path)
        else:
            print('Loading pretrained weights for autoencoder.')
            self.autoencoder.load_state_dict(torch.load(self.pretrained_weights))
        
        # update encoder, decoder
        
        self.encoder = Encoder(self.autoencoder)
    
        print('Initializing cluster centres with Lloyd.')
        
        if self.cluster_centres is None:
            siamese_dataset = SiameseDataset(v_train, zsl=self.zsl, num=400)
            data_loader = DataLoader(siamese_dataset, batch_size=400, shuffle=True)
            kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)
            for x1, x2, _ in data_loader:
                break
            with torch.no_grad():
                z1 = self.encoder(x1.unsqueeze(1))
                z2 = self.encoder(x2.unsqueeze(1))
                result = torch.abs(z1 - z2).cpu().numpy()
            
            self.y_pred = kmeans.fit_predict(result)
            self.cluster_centres = kmeans.cluster_centers_

            self.marker_layer = MarkerLayer(output_dim=2, pos_markers=self.pos_markers, markers=self.n_clusters, weights=self.cluster_centres, alpha=self.alpha)

            self.siamese_network = SiameseNetwork(encoder=self.encoder, autoencoder=self.autoencoder, cluster_layer=self.marker_layer)

        if self.verbose != 0:
            self.summary()
        
        self.criterion = nn.BCELoss()
        self.optimizer = optim.SGD(self.siamese_network.parameters(), lr=0.01, momentum=0.9)

        return
        
    def summary(self):
        print('Encoder:')
        print(self.encoder)
        print('Marker Layer:')
        print(self.marker_layer)
        print('Siamese Network:')
        print(self.siamese_network)
    
    

    def trainModule(self, v_train, v_test, N=10, train_pairs=3550, num_epochs=320, save_path_name="siamese_network", log_file='log.txt'):
        
        print('Training SMELL')
        with open(log_file, 'w') as f:
            f.write('Training SMELL\n')
            f.close()

        print(f'Number of epochs: {num_epochs}')
        with open(log_file, 'a') as f:
            f.write(f'Number of epochs: {num_epochs}\n')
            f.close()

        
        train = True
        iteration = 0
        self.accuracy = []
        acc_m = 0


        # self.siamese_network.load_state_dict(torch.load('./siamese_network_x86_64_check.pth'))
        
        for epoch in range(num_epochs):
                
            siamese_dataset = SiameseDataset(v_train, zsl=self.zsl, num=train_pairs)
            data_loader = DataLoader(siamese_dataset, batch_size = self.batch_size, shuffle=True)
            self.siamese_network.train()
            for i, data in enumerate(data_loader):
                X1, X2, y = data
                y = torch.tensor(y, dtype=torch.float)
                outputs = self.siamese_network(X1.unsqueeze(1), X2.unsqueeze(1))
                outputs = outputs[1].ravel()
                loss = self.criterion(outputs, y)
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
            
            if (epoch+1) % 10 == 0:
                siamese_dataset = SiameseDataset(v_test, num=N)
                data_loader = DataLoader(siamese_dataset, batch_size = self.batch_size, shuffle=False)
                self.siamese_network.eval()
                correct = 0
                total = 0
                with torch.no_grad():
                    for data in data_loader:
                        X1, X2, y = data
                        y = torch.tensor(y, dtype=torch.float)
                        outputs  = self.siamese_network(X1.unsqueeze(1), X2.unsqueeze(1))
                        predicted = outputs.argmax(dim=0)
                        total += y.size(0)
                        correct += (predicted == y).sum().item()
                acc = 100 * correct / total
                self.accuracy.append(acc)
                print(f'Epoch {epoch+1}, Estimated Accuracy: {acc:.5f}, Loss: {loss:.5f}')
                with open(log_file, 'a') as f:
                    f.write(f'Epoch {epoch+1}, Estimated Accuracy: {acc:.5f}, Loss: {loss:.5f}\n')
                    f.close()
                self.cluster_centres = self.marker_layer.W.data.numpy()

                torch.save(self.siamese_network.state_dict(), f'./{save_path_name}_check.pth')

                if acc_m <= self.accuracy[-1]:
                    acc_m = self.accuracy[-1]
                    torch.save(self.siamese_network.state_dict(), f'./{save_path_name}.pth')
                    print('Model saved')
                    with open(log_file, 'a') as f:
                        f.write('Model saved\n')
                        f.close()

        print('Reached maximum iteration limit. Stopping training.')
        with open(log_file, 'a') as f:
            f.write('Reached maximum iteration limit. Stopping training.\n')
            f.close()
        return self.y_pred
    
    
    def predict_fsl(self, v_train, x_test, y_test, k=3, size_fsl=5, weight=None):

        if weight is not None:
            self.siamese_network.load_state_dict(torch.load(weight))
            state_dict = self.siamese_network.state_dict()
            for name, param in self.encoder.named_parameters():
                if name in state_dict:
                    param.data.copy_(state_dict[name])

        x_train_fsl = torch.tensor([])
        y = torch.tensor([], dtype=torch.long)
        for y_id, e in enumerate(v_train):
            if y_id == 0:
                x_train_fsl = e[:size_fsl]
                y = np.array([y_id for e in range(size_fsl)])
            else:
                x_train_fsl = np.concatenate((x_train_fsl, e[:size_fsl]))
                y = np.concatenate((y, np.array([y_id for e in range(size_fsl)])))
        
        y_labels = set(y)
        y_pred = []

        batch_size = 75  # Batch step for prediction to avoid memory overflow

        for idx in tqdm(range(len(x_test))):
            
            
            r = [[] for _ in range(len(x_train_fsl))]
            predict = x_test[idx]
            dataset = SiameseDatasetTest(x_train_fsl, predict)
            dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=False)

            for batch_idx, (x_train_batch, predict_batch) in enumerate(dataloader):
                x_train_batch = x_train_batch.unsqueeze(1)
                predict_batch = predict_batch.unsqueeze(1)
                self.siamese_network.eval()

                with torch.no_grad():
                    q = self.siamese_network(x_train_batch, predict_batch).numpy()

                start_idx = batch_idx * batch_size

                for i in range(len(x_train_fsl)):
                    r[start_idx + i] = [q[1][i], y[start_idx + i]]

            r.sort(reverse=True)
            # print(r)
            r = r[:k]
            s = np.zeros(len(y_labels))
            for e in r:
                s[e[1]] += 1
            y_pred.append(np.argmax(s))

        return y_pred, accuracy_score(y_test, y_pred)

    def predict_zsl(self, v_train, x_test, y_test, k=3, size_fsl=5, weight=None):
        
        marker = MarkerLayer_ZSL(output_dim=2, pos_markers=3, markers=self.n_clusters, weights=self.cluster_centres, alpha=self.alpha)
        state_dict = self.marker_layer.state_dict()
        marker.load_state_dict(state_dict)
        model = SiameseNetwork(encoder=self.encoder, autoencoder=self.autoencoder, cluster_layer=marker)
        state_dict = self.siamese_network.state_dict()
        model.load_state_dict(state_dict)

        if weight is not None:
            model.load_state_dict(torch.load(weight))
            state_dict = model.state_dict()
            for name, param in self.encoder.named_parameters():
                if name in state_dict:
                    param.data.copy_(state_dict[name])

            

        x_train_fsl = torch.tensor([])
        y = torch.tensor([], dtype=torch.long)
        for y_id, e in enumerate(v_train):
            if y_id == 0:
                x_train_fsl = e[:size_fsl]
                y = np.array([y_id for e in range(size_fsl)])
            else:
                x_train_fsl = np.concatenate((x_train_fsl, e[:size_fsl]))
                y = np.concatenate((y, np.array([y_id for e in range(size_fsl)])))
        
        y_labels = set(y)
        y_pred = []

        batch_size = 75  # Batch step for prediction to avoid memory overflow

        for idx in tqdm(range(len(x_test))):
            
            
            r = [[] for _ in range(len(x_train_fsl))]
            predict = x_test[idx]
            dataset = SiameseDatasetTest(x_train_fsl, predict)
            dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=False)

            for batch_idx, (x_train_batch, predict_batch) in enumerate(dataloader):
                x_train_batch = x_train_batch.unsqueeze(1)
                predict_batch = predict_batch.unsqueeze(1)
                model.eval()

                with torch.no_grad():
                    dis = model(x_train_batch, predict_batch).numpy()
                print(dis)


        return y_pred, accuracy_score(y_test, y_pred)